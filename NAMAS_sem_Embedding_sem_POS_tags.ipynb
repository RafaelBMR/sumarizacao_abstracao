{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "from keras.layers import Input, Embedding, Dense, AveragePooling1D, Dot, Softmax, Multiply, Add, Flatten, Reshape\n",
    "from keras.layers import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open( name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def save_obj(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def montar_sentenca(indices_sentenca):\n",
    "    indices = np.zeros((43), dtype=np.int16)\n",
    "    i = 0\n",
    "    while i < len(indices_sentenca):\n",
    "        indices[i] = indices_sentenca[i]\n",
    "        i += 1\n",
    "    return indices\n",
    "\n",
    "def montar_bloco(exemplos_batch):\n",
    "    sentencas_indices_tokens = []\n",
    "    sentencas_indices_tags = []\n",
    "    contextos_indices_tokens = []\n",
    "    saidas = []\n",
    "    for exemplo in exemplos_batch:\n",
    "        indices_sentenca = montar_sentenca(exemplo['indices_sentenca'])\n",
    "        contexto_indices_tokens = np.zeros((4), dtype=np.int16)\n",
    "        i = 0\n",
    "        while i < len(exemplo['indices_manchete']) - 1:\n",
    "            if i < 4:\n",
    "                contexto_indices_tokens[i] = exemplo['indices_manchete'][i]\n",
    "            else:\n",
    "                for j in range(3):\n",
    "                    contexto_indices_tokens[j] = contexto_indices_tokens[j+1]\n",
    "                contexto_indices_tokens[3] = exemplo['indices_manchete'][i]\n",
    "            indice_saida = exemplo['indices_manchete'][i+1]\n",
    "            if indice_saida != token2ind_manchetes['<UNK>']:\n",
    "                saida = np.zeros((len(ind2token_manchetes)))\n",
    "                saida[indice_saida] = 1\n",
    "                sentencas_indices_tokens.append(indices_sentenca)\n",
    "                contextos_indices_tokens.append(contexto_indices_tokens.copy())\n",
    "                saidas.append(saida)\n",
    "            i+=1\n",
    "    sentencas_indices_tokens = np.array(sentencas_indices_tokens, dtype=np.int16)\n",
    "    contextos_indices_tokens = np.array(contextos_indices_tokens, dtype=np.int16)\n",
    "    saidas = np.array(saidas, dtype=np.int16)\n",
    "    return sentencas_indices_tokens, contextos_indices_tokens, saidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquivos Necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2token_manchetes = load_obj(\"ind2token_manchetes.pkl\")\n",
    "token2ind_manchetes = load_obj(\"token2ind_manchetes.pkl\")\n",
    "ind2token_sentencas = load_obj(\"ind2token_sentencas.pkl\")\n",
    "token2ind_sentencas = load_obj(\"token2ind_sentencas.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurações do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 43\n",
    "C = 4\n",
    "Q = 2\n",
    "tamanho_embedding_sentenca = 200\n",
    "tamanho_embedding_contexto = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Entradas\n",
    "sentenca_entrada = Input(shape=(M,))\n",
    "contexto_entrada = Input(shape=(C,))\n",
    "embeddings_sentenca = Embedding(len(ind2token_sentencas), tamanho_embedding_sentenca,\n",
    "                               trainable=True, \n",
    "                                name=\"embeddings_sentenca\")(sentenca_entrada)\n",
    "embeddings_contexto_encoder = Embedding(len(ind2token_manchetes), tamanho_embedding_contexto,\n",
    "                                       trainable=True, \n",
    "                                       name=\"embeddings_contexto_encoder\")(contexto_entrada)\n",
    "embeddings_contexto_decoder = Embedding(len(ind2token_manchetes), tamanho_embedding_contexto,\n",
    "                                       trainable=True,\n",
    "                                       name=\"embeddings_contexto_decoder\")(contexto_entrada)\n",
    "\n",
    "# Codificador\n",
    "bow_contexto_encoder = Reshape((1,4 * tamanho_embedding_contexto))(embeddings_contexto_encoder)\n",
    "pesos_multiplicacao = Dense(4 * tamanho_embedding_contexto)(embeddings_sentenca)\n",
    "multiplicacao_sentenca_contexto = Dot(axes=2)([pesos_multiplicacao, bow_contexto_encoder])\n",
    "atencao_sobre_entrada = Softmax(axis=1)(multiplicacao_sentenca_contexto)\n",
    "smoothed_window = AveragePooling1D(pool_size=2*Q+1, strides=1, padding=\"same\")(embeddings_sentenca)\n",
    "smoothed_window_com_pesos = Multiply()([atencao_sobre_entrada, smoothed_window])\n",
    "codificacao_sentenca = AveragePooling1D(pool_size = M)(smoothed_window_com_pesos)\n",
    "\n",
    "# Decodificador\n",
    "bow_contexto_decoder = Reshape((1, 4*tamanho_embedding_contexto))(embeddings_contexto_decoder)\n",
    "codificacao_contexto = Dense(tamanho_embedding_sentenca, activation='tanh')(bow_contexto_decoder)\n",
    "\n",
    "# Classificação\n",
    "classificador_sentenca = Dense(len(ind2token_manchetes))(codificacao_sentenca)\n",
    "classificador_contexto = Dense(len(ind2token_manchetes))(codificacao_contexto)\n",
    "distribuicao_probabilidade = Softmax()(Flatten()((Add()([classificador_sentenca, classificador_contexto]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[sentenca_entrada, contexto_entrada], outputs=distribuicao_probabilidade)\n",
    "sgd = SGD(lr=0.05)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurações de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 16\n",
    "tamanhos_treinamento = list(range(23,44))\n",
    "losses_treinamento = []\n",
    "losses_validacao = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inicio = dt.now()\n",
    "for epoch in range(num_epochs):\n",
    "    # Embaralha os tamanhos\n",
    "    np.random.shuffle(tamanhos_treinamento)\n",
    "    # Carrega cada bloco e treina\n",
    "    losses_treinamento_epoch = []\n",
    "    print(\"Início do treino epoch \", str(epoch))\n",
    "    for tamanho in tamanhos_treinamento:\n",
    "        exemplos = load_obj(\"exemplos_treinamento_\" + str(tamanho) + \".pkl\")\n",
    "        sentencas, contextos, saidas = montar_bloco(exemplos)\n",
    "        history = model.fit(x=[sentencas, contextos], y=saidas, batch_size=batch_size, verbose=0)\n",
    "        losses_treinamento_epoch.append(history.history['loss'])\n",
    "        \n",
    "    loss_treinamento = np.mean(losses_treinamento_epoch)\n",
    "    losses_treinamento.append(loss_treinamento)\n",
    "    # Validação\n",
    "    losses_validacao_epoch = []\n",
    "    print(\"Início da validação epoch \", str(epoch))\n",
    "    for tamanho in tamanhos_treinamento:\n",
    "        exemplos_validacao = load_obj(\"exemplos_validacao_\" + str(tamanho) + \".pkl\")\n",
    "        sentencas, contextos, saidas = montar_bloco(exemplos_validacao)\n",
    "        losses_validacao_epoch.append(model.evaluate(x=[sentencas, contextos], y=saidas, verbose=0))\n",
    "    loss_validacao = np.mean(losses_validacao_epoch)\n",
    "    losses_validacao.append(loss_validacao)\n",
    "    print(\"Epoch \", str(epoch + 1), \". Loss treinamento: \", str(loss_treinamento), \". Loss validação: \", \n",
    "          str(loss_validacao), \"\\nTempo total: \", str(dt.now() - inicio))\n",
    "    model.save(\"salvar/model_sem_embeddings_sem_pos_tags_\" + str(epoch) + \".h5\")\n",
    "    model.save_weights(\"salvar/model_sem_embeddings_sem_pos_so_pesos_\" + str(epoch) + \".h5\")\n",
    "    save_obj(losses_validacao, \"salvar/losses_validacao_\" + str(epoch) + \".pkl\")\n",
    "    save_obj(losses_treinamento, \"salvar/losses_treinamento_\" + str(epoch) + \".pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
